<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="Jiaxin Shi">

  <title>LMTuner Homepage</title>

  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/jointjs/2.1.0/joint.css" />

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link
    href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
    rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/clean-blog.min.css" rel="stylesheet">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLKG3T9QJ2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-QLKG3T9QJ2');
  </script>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="index.html">LMTuner</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="index.html">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/WENGSYX/LMTuner">Code</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://arxiv.org/abs/2304.01665">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="install.html">Install</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/WENGSYX/LMTuner/blob/main/QA/readme.md">中文简体</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Header -->
  <header class="masthead" style="background-image: url('img/数据集.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="site-heading">
            <h1>Install</h1>
            <span class="subheading">To facilitate training large language models more easily, we developed the LMTuner system. You just need to follow our method to easily configure the environment on the server..</span>
          </div>
        </div>
      </div>
    </div>
  </header>


  <!-- Main Content -->
  <div class="container-fluid">
    <div class="block-light">
      <div class="row">
        <div class="col-md-8 mx-auto">
          <h1>How to Install the Required Environment for LMTuner from Scratch on a Server</h1>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-7 mx-auto" style="text-align: left; font-size: 15px;">

          <h5>Install GPU Driver</h5>

          <pre><code class="language-bash">
          # preparing environment
          sudo apt-get install gcc
          sudo apt-get install make
          wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run
          sudo sh cuda_11.5.1_495.29.05_linux.run
          </code></pre>
        </div>
      </div>

      <div class="row">
        <div class="col-md-7 mx-auto" style="text-align: left; font-size: 15px;">

          <h5>Install Conda and Python</h5>

          <pre><code class="language-bash">
            wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
            sudo chmod 777 Miniconda3-latest-Linux-x86_64.sh
            bash Miniconda3-latest-Linux-x86_64.sh

            conda create -n LMTuner python==3.9
            conda activate LMTuner
          </code></pre>
        </div>      </div>

      <div class="row">
        <div class="col-md-7 mx-auto" style="text-align: left; font-size: 15px;">

          <h5>Install Python Libraries</h5>

          <pre><code class="language-bash">
            pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117
            pip install tqdm transformers sklearn pandas numpy accelerate sentencepiece wandb SwissArmyTransformer jieba rouge_chinese datasets

          </code></pre>
        </div>
      </div>


        <div class="row">
        <div class="col-md-7 mx-auto" style="text-align: left; font-size: 15px;">

          <p>If you have ensured installing the correct GCC library (>5.0.0), you can continue installing apex and deepspeed:</p>
          <pre><code class="language-bash">
            git clone https://github.com/NVIDIA/apex
            cd apex
            pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
            pip install deepspeed
          </code></pre>
        </div>
      </div>


            <div class="row">
        <div class="col-md-7 mx-auto" style="text-align: left; font-size: 15px;">

          <h5>Install LMTuner</h5>

          <pre><code class="language-bash">
            git clone https://github.com/WENGSYX/LMTuner
            pip install .
          </code></pre>
        </div>
      </div>

    </div>
  </div>
      <br>

      <div class="row">
        <div class="col-md-8 mx-auto">
          <h3>System Requirements :</h3>
          <pre id="pred_sparql" class="code-center"> </pre>
          <b id='pred_result_by_sparql'></b>
          <ol>
          <li>Ubuntu 14.04+, Debian 8+, CentOS 6+, or Fedora 27+</li>
          <li>an NVIDIA GPU with driver version >= 460.32.03 or AMD GPU with ROMc >= 4.0</li>
          </ol>
        </div>
      </div>

        <br><hr>
      <br>

      <div class="row">
        <div class="col-md-8 mx-auto">
          <h3>Error in Apex installation :</h3>
          <pre id="pred_sparql" class="code-center"> </pre>
          <b id='pred_result_by_sparql'></b>
          <ol>
          <li>If prompted "Pytorch binaries were compiled with Cuda“, it means the PyTorch version may not match cuda, but this will not affect the installation of apex, so replace line 32 of the apex setup.py file "if (bare_metal_version != torch_binary_version):" with "if 0:"</li>
          <li>No module named 'packaging': pip install packaging</li>
          <li>ninja: build stopped: subcommand failed.: pip install ninja</li>
          <li>subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.: modify the cpp file in torch to ninja -V</li>
          <li>g++: error: /home/wengyixuan/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense.o: No such file or directory: Give up, incompatible, either the torch environment or the pytorch environment or the apex environment is wrong. It is recommended to change to other versions, such as python=3.8, torch==1.7/1.8 or historical versions of apex (because newer versions may be updated in the future)</li>
          </ol>
        </div>
      </div>

        <br><hr>

      <div class="row">
        <div class="col-md-8 mx-auto">
          <h3>Common Issues</h3>
          <pre id="pred_sparql" class="code-center"> </pre>
          <b id='pred_result_by_sparql'></b>
<ol>
<li>OOM (out of memory): Reduce batch size, use gradient accumulation or adjust model parameters.</li>
<li>Gradient explosion/vanishing: Use gradient clipping, adjust learning rate properly.</li>
<li>Overfitting: Consider more regularization (e.g. L2 regularization, dropout etc), use larger dataset.</li>
<li>Low GPU utilization: Check data loading and batch sizes, avoid small batches.</li>
<li>Mixed precision training issues: Ensure mixed precision enabled, check numerical stability, use gradient scaling.</li>
<li>Network communication bottleneck: Optimize data loading, use dedicated libs (like NCCL) and high-speed interconnects.</li>
<li>Unable to Use Single Node Multi-GPU: May be due to p2p transmission, set NCCL_P2P_DISABLE = 1</li>
  <li>Learning Rate is 0 During Training: May be gradient vanishing, remember to check model, optimizer, etc. And use Gaussian initialization for weights.</li>
</ol>
        </div>
      </div>

  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <!--             <li class="list-inline-item">
              <a href="#">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="#">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li> -->
            <li class="list-inline-item">
              <a href="#">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
          <p class="copyright text-muted">Copyright &copy; WENGSYX 2023</p>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/clean-blog.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/dagre/0.8.5/dagre.min.js"></script>
  <script src="js/nomnoml.js"></script>

</body>

</html>